# Direct-Preference-Optimization: The Ultimate RLHF

This is a method of fine tuning that is actually similar to contamination. In giving a model a list of chosen vs. rejected responses, you will teach the model to generate responses that are more like the chosen ones.

[A complete guide on DPO can be found here](https://towardsdatascience.com/fine-tune-a-mistral-7b-model-with-direct-preference-optimization-708042745aac), thanks for Mlabonne.