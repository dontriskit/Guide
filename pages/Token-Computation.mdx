---
title: "Large Language Models: Understanding the Process of Computing Output Tokens"
---

In this article, we will discuss the entire process of how large language models compute output tokens. This article is not meant to be a deep dive into the complex math but rather an overview of the concepts of each component of the large language model. We will start with an overview of some important concepts, move on to tokenization, embedding spaces, and finally attention, and how it all comes together to actually compute the output tokens.

## Key Concepts

Before we dive into the steps involved in computing output tokens, it is important to understand a few key concepts that are used throughout the process. These concepts include:

- **Softmax:** Softmax is a mathematical function that converts a vector of numbers into a vector of probabilities. It is used in multi-head attention and when computing output logits. By constraining the values before outputting them between 0 and 1, it helps the model learn more effectively.

- **Layer Normalization:** Layer normalization is an intermediate state that ensures that the values across a sample sum to one. This helps to maintain the scale of the values as they pass through different layers of the model.

- **Matrices:** Matrices do not impart positional information. This means that the order of the rows in a matrix does not affect the matrix itself. This is important when working with sequences of tokens.

- **Feed Forward Layers:** Feed forward layers are multi-perceptron layers that massage data before they go between layers. They are used throughout large language models to process data in different ways.

## Tokenization

Tokenization is the first step in the large language model process. It involves breaking down the input string into tokens that the model can understand. There are three primary types of tokenization:

- **Word-wise:** In word-wise tokenization, every word in the language becomes a token. However, this can lead to massive vocabularies and does not allow the model to handle misspellings or allow for flexibility in how it learns to handle completions.

- **Character-wise:** In character-wise tokenization, every character is a token. This has the inverse issue of word-wise tokenization, where the vocabulary is very small and the model has to learn how to combine all of the different characters into possible words.

- **Subword tokenization:** Subword tokenization is a compromise between the other two methods. It allows for flexibility in handling misspellings and gives the model a lot of flexibility in how it learns to handle completions. The most common type of this method is byte-pair encoding, which is an iterative process where new pairings are created for each iteration.

Once the input string has been tokenized, it is broken out into the tokens that the model can understand. These tokens are then turned into numeric representations, known as token embeddings, that the model can work with.

## Embedding Spaces

After tokenization, the next step is to create an embedding space. An embedding space is a high-dimensional space where points represent tokens. The closer the points are to each other, the more related they are. For example, the names Jeff, Alex, and Sam might all be close to each other in the embedding space, since they represent names.

Embedding spaces are typically very high-dimensional, with up to 764 or even 12,000 dimensions or more. The model understands how things are related in the embedding space through metrics like Euclidean distance or cosine similarity.

## Attention

The final step in computing output tokens is attention. Attention allows the model to consider context when it's completing sentences for us. It is meant to emulate how humans solve problems by taking context clues into account.

Attention is computed using query, key, and value matrices. These matrices are calculated by linear transformations that are learned during the training process. The query matrix represents the type of movie we want to watch, the key matrix represents metadata about all possible movies, and the value matrix represents the possible movies themselves.

By multiplying the query and key matrices, we can compute an attention score that tells the model what it is really considering in the context. This attention score can then be used to manipulate the output values to start computing what the most likely next token is.
